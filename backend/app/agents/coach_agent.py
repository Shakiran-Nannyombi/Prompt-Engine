import sys
import os
import language_tool_python
from dotenv import load_dotenv
import psycopg
from typing import TypedDict, Annotated, List
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_groq import ChatGroq
from streamlit import feedback
from models.evaluation import EvaluationResult
from langgraph.prebuilt import ToolNode
from agents.tools import coach_tools

sys.path.append(os.path.abspath(".."))

# Importing environment variables (load from project root if available)
load_dotenv()

# Setup PostgreSQL checkpointing
DB_URI = os.environ.get("DATABASE_URL", "")
if not DB_URI:
    raise RuntimeError("DATABASE_URL is not set in environment")

# State of graph
class CoachingState(TypedDict):
    task: str
    context: str
    references: List[str]
    evaluation_criteria: str
    final_prompt: str
    messages: Annotated[list, add_messages]
    current_step: str
    evaluation_result: EvaluationResult
    summary: str
    
# Models
llm = ChatGroq(
    model="llama-3.1-8b-instant", 
    temperature=0.5
)

tool_node = ToolNode(coach_tools.tool_list)
evaluation_llm = llm.with_structured_output(EvaluationResult)
llm_with_tools = llm.bind_tools(tools=coach_tools.tool_list, parallel_tool_calls=False)

welcome_message = """
**Welcome!** I'm here to help you master prompt engineering by building an effective 
prompt using the **'Task(Persona and Format), Context, References, Evaluate, Iterate'** framework.

To get more understanding of this framework, you can refer to our documentation
[here](https://example.com/docs).

**Let's start with the Task.** Please describe what you would like to accomplish.
It could be a specific goal or a general area of interest. 

**Don't have anything in mind?** Just ask me to "suggest some tasks" and I'll generate creative ideas for you!
"""

def start_coaching(state: CoachingState):
    existing_messages = state.get("messages", [])
    if existing_messages:
        # Preserve current step or default to awaiting_task_input
        return {"current_step": state.get("current_step", "awaiting_task_input")}
    return {
        "messages": [AIMessage(content=welcome_message)], 
        "current_step": "awaiting_task_input"
    }

def process_task_input(state: CoachingState):
    messages = state.get("messages", [])
    if not messages or not isinstance(messages[-1], HumanMessage):
        # Should never happen with proper routing, but just in case
        return {"current_step": "error"}
    
    last_message = messages[-1]
    user_input = last_message.content.lower()

    # Check if it's just a greeting or very short input
    if len(user_input.strip()) < 5 or user_input.strip() in ["hi", "hello", "hey", "sup", "yo"]:
        greeting_response = f"""
        Hello! ðŸ‘‹ I'm excited to help you create an effective prompt!
        
        To get started, please tell me what you'd like to accomplish. For example:
        - "Create a chatbot for tutoring students in math"
        - "Write a professional email template"
        - "Generate code for a simple calculator"
        - "Design a lesson plan for teaching history"
        
        **What would you like to create or build?**
        """
    
        return {
            "messages": [AIMessage(content=greeting_response)], 
            "current_step": "awaiting_task_input"
        }
        
        # Check if user wants suggestions
    if any(keyword in user_input for keyword in ["suggest", "suggestion", "random", "idea", "help me think"]):
        suggestion_prompt = """
        Generate 4 creative and diverse task ideas for prompt engineering practice. 
        Each task should be practical, engaging, and cover different domains.
        Format your response as a numbered list with brief descriptions.
        """
        response = llm.invoke(suggestion_prompt)
        
        suggestion_message = f"""
        **Here are some creative task suggestions for you:**
        {response.content}
        
        **Choose one of these or describe your own task!**
        Which of these interests you, or would you like to tell me about a different task you have in mind?
        """
        return {
            "messages": [AIMessage(content=suggestion_message)], 
            "current_step": "awaiting_task_input"
        }
    
    # Evaluate the task
    instruction = f"""
    Evaluate the following user-defined task based on clarity, specificity, and actionability.
    Check if they have included a persona and output format.
    User's Task: "{last_message.content}"
    
   ACCEPT the task if it describes:
    - Any type of content creation (essays, stories, code, etc.)
    - Any kind of assistant or chatbot functionality
    - Any educational or tutoring request
    - Any analysis, research, or information gathering task
    - Any creative or technical project
    
    ONLY REJECT if the input is:
    - Completely unrelated to a task (like just "hi" or "hello")
    - Harmful, inappropriate, or illegal content
    - Completely nonsensical
    
    Be ENCOURAGING and HELPFUL. Most user inputs should be accepted as valid starting points.
    If the task is somewhat vague but shows clear intent, ACCEPT it and suggest improvements in the feedback.
    
    For the task: "{last_message.content}"
    - Does this describe something the user wants to accomplish? 
    - Is it a reasonable request for AI assistance?
    
    If YES, mark as correct and provide encouraging feedback.
    If the task could be more specific, still mark as correct but suggest enhancements.
    """
    
    result: EvaluationResult = evaluation_llm.invoke(instruction) 
    # Preserve user's exact task when accepted; only use updated_prompt when incorrect
    final_task = last_message.content
    
    if result.is_correct:
        # Require output format; persona is optional
        format_line = "Now lets add some format to the task (e.g., Markdown table, bullet list, JSON, CSV) or any way you want your output to be."
        persona_line = "Persona is optional (e.g., 'You are an academic advisor')."
        context_prompt = f"""
        Perfect! I understand your task: **"{final_task}"**
        
        {format_line}
        {persona_line}
        
        Now let's add some context to make your prompt even more effective.
        Please provide context by thinking about:
        - **Background** (setting or situation)
        - **Audience** (who is this for?)
        - **Requirements** (constraints, length, style)
        - **Purpose** (how will this be used?)
        
        **Share your context when ready.**
        """
        
        # Ensures we progress to context step
        return {
            "task": final_task,
            "evaluation_result": result,
            "messages": [AIMessage(content=context_prompt)],
            "current_step": "awaiting_context_input"
        }
    else:
        # Only reject clearly invalid inputs
        feedback_message = f"""
        I'd like to help, but I need a clearer understanding of what you want to accomplish.
        
        {result.feedback}
        
        **Examples of tasks I can help with:**
        - "Create a chatbot for customer service"
        - "Write an essay about climate change"  
        - "Generate code for a calculator app"
        - "Design a lesson plan for teaching math"
        
        **What would you like to create or accomplish?**
        """
        return {
            "task": final_task,
            "evaluation_result": result,
            "messages": [AIMessage(content=feedback_message)],
            "current_step": "awaiting_task_input"
        }

def process_context_input(state: CoachingState):
    messages = state.get("messages", [])
    if not messages or not isinstance(messages[-1], HumanMessage):
        return {"current_step": "error"}
    
    last_message = messages[-1]
    task = state.get("task", "")
    
    instruction = f"""
    Evaluate the following user-defined context based on relevance, completeness, and clarity.
    Analyze how well it supports the task: "{task}"
    User's Context: "{last_message.content}"
    Is this context description sufficient to proceed?
    """
    
    result: EvaluationResult = evaluation_llm.invoke(instruction) 
    # Preserve user's context when accepted
    final_context = last_message.content
    
    if result.is_correct:
        reference_prompt = f"""
        Excellent! Your context adds great depth: "{final_context}"
        
        Now let's gather references. Please think about:
        - Documents, links, or resources that could help
        - Examples of similar tasks or prompts
        - Specific data or information to include
        - Style guides or formatting preferences
        
        **Please describe any references or additional resources for your task.**
        """
        
        return {
            "context": final_context,
            "evaluation_result": result,
            "messages": [AIMessage(content=reference_prompt)],
            "current_step": "awaiting_reference_input"
        }
    else:
        checklist = f"""
        Your context can mention:
        - Courses, audience, level, constraints (e.g., hours/week)
        - Key goals and priorities
        - Any constraints or deadlines
        
        Examples tailored to your task:
        1) "I'm a 3rd-year CS student taking AI, UI, Embedded Systems. I have 20 hours/week. I need a weekly plan with exam countdowns."
        2) "I work 15 hours/week part-time. Prefer a Markdown table with days Monâ€“Sun, morning/evening slots, and focus sessions for metrics and evolution."
        """
        feedback_message = f"{result.feedback}\n\nPlease provide more detailed context and try again.\n\n{checklist}\n\nPlease share your context, then weâ€™ll proceed to references."
        return {
            "context": final_context,
            "evaluation_result": result,
            "messages": [AIMessage(content=feedback_message)],
            "current_step": "awaiting_context_input"
        }

def process_reference_input(state: CoachingState):
    messages = state.get("messages", [])
    if not messages or not isinstance(messages[-1], HumanMessage):
        return {"current_step": "error"}
    
    last_message = messages[-1]
    task = state.get("task", "")
    context = state.get("context", "")
    user_refs = (last_message.content or "").strip().lower()

    # If user explicitly has no references, provide tailored suggestions instead of erroring
    no_ref_patterns = [
        "no references", "none", "don't have", "dont have", "i have none", "no ref", "no resource", "nothing"
    ]
    if any(p in user_refs for p in no_ref_patterns):
        suggestion_prompt = f"""
        Based on the user's task and context, suggest practical reference ideas the user could provide
        to improve prompt quality. Tailor to their scenario.
        Task: "{task}"
        Context: "{context}"

        Return 5-7 concise, concrete suggestions in a numbered list. Cover:
        - example datasets or documents they may already have
        - similar examples/templates from their domain
        - links or sections they could look up (generic placeholders ok)
        - style/tone guides relevant to their audience
        - any structured info (tables/fields) helpful for this task
        Keep items short (one line each).
        """
        # Combine LLM suggestions with web search suggestions (non-authoritative)
        response = llm.invoke(suggestion_prompt)
        search_query = f"reference ideas for: {task} ({context[:60]}) study planner"
        try:
            web_results = coach_tools.tavily_search_tool.run(search_query)
        except Exception:
            web_results = ""

        suggestion_message = f"""
        Itâ€™s okay if you donâ€™t have references yet. Here are some ideas to consider:

        {response.content}

        Web suggestions (to explore):
        {web_results}

        You can:
        - Pick any items from the list to use as references
        - Describe informal references (notes, past assignments, screenshots)
        - Or reply "proceed without references" and we'll continue
        """

        return {
            "messages": [AIMessage(content=suggestion_message)],
            "current_step": "awaiting_reference_input"
        }
    
    instruction = f"""
    Evaluate the following user-defined references based on relevance, credibility, and usefulness.
    Task: "{task}", Context: "{context}", References: "{last_message.content}"
    Is this reference description sufficient to proceed?
    """
    
    result: EvaluationResult = evaluation_llm.invoke(instruction) 
    # Preserve user's references when accepted
    final_references = last_message.content
    
    if result.is_correct:
        summary = f"""
        **Task:** {task}
        **Context:** {context}
        **References:** {final_references}
        """
        
        final_prompt_guidance = f"""
        Perfect! You have all the key components:
        {summary}
        
        **Now create your final prompt** that incorporates:
        - Your task (with clear persona/role and output format)
        - The context you've provided
        - Reference to the resources you mentioned
        - Clear, specific instructions
        
        **Write your complete engineered prompt below:**
        """
        
        return {
            "references": [final_references],
            "evaluation_result": result,
            "summary": summary,
            "messages": [AIMessage(content=final_prompt_guidance)],
            "current_step": "awaiting_final_prompt"
        }
    else:
        feedback_message = f"{result.feedback}\n\nPlease provide better references and try again."
        return {
            "references": [final_references],
            "evaluation_result": result,
            "messages": [AIMessage(content=feedback_message)],
            "current_step": "awaiting_reference_input"
        }

def process_final_prompt(state: CoachingState):
    messages = state.get("messages", [])
    if not messages or not isinstance(messages[-1], HumanMessage):
        return {"current_step": "error"}
    
    last_message = messages[-1]
    summary = state.get("summary", "")
    
    evaluation_instruction = f"""
    Evaluate this final prompt for clarity, completeness, and effectiveness:
    Framework used: {summary}
    Final prompt: "{last_message.content}"
    
    Is this prompt well-structured and ready to use?
    """
    
    result: EvaluationResult = evaluation_llm.invoke(evaluation_instruction)
    
    if result.is_correct:
        # brief rubric and compatibility text for tests
        rubric = "- Clarity: âœ“  - Constraints: âœ“/âœ—  - Format specified: âœ“/âœ—  - References used: âœ“/âœ—"
        preface = f"Your prompt looks solid. Here's a quick rubric check:\n{rubric}\nExcellent prompt! Let me polish it for you..."
        return {
            "final_prompt": last_message.content,
            "messages": [AIMessage(content=preface)],
            "current_step": "ready_to_refine"
        }
    else:
        feedback_message = f"{result.feedback}\n\nPlease refine your prompt based on this feedback."
        return {
            "evaluation_result": result,
            "messages": [AIMessage(content=feedback_message)],
            "current_step": "awaiting_final_prompt"
        }

def agent_node(state: CoachingState):
    # Agent node acts as the brain that uses tools to refine the final prompt
    final_prompt = state.get("final_prompt", "")
    refine_instruction = f"Please use the grammar_checker tool to polish this prompt: {final_prompt}"
    
    messages = state.get("messages", [])
    messages.append(SystemMessage(content=refine_instruction))
    
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def should_call_tools(state: CoachingState) -> str:
    # Checks if we should call tools
    messages = state.get("messages", [])
    if messages:
        last_message = messages[-1]
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "call_tools"
    return "display_final_result"

def display_final_result(state: CoachingState):
    final_prompt = state.get("final_prompt", "")
    
    completion_message = f"""
**Congratulations!** You've successfully crafted a well-structured prompt using the 
**'Task, Context, References, Evaluate, Iterate'** framework.

**Your Final Engineered Prompt:**

---
{final_prompt}
---

**What you've accomplished:**
- Defined a clear task with persona and format
- Provided relevant context 
- Included helpful references
- Practiced evaluation and iteration throughout

You can now use this prompt with your AI model or save it to your prompt library.

**Happy Prompting!** 
    """
    
    return {
        "messages": [AIMessage(content=completion_message)],
        "current_step": "completed"
    }

# Route user input to the appropriate processing node based on current step
def decide_next_step(state: CoachingState) -> str:
    current_step = state.get("current_step", "")
    messages = state.get("messages", [])
    # Checking for the existing updated state
    has_task = bool(state.get("task"))
    has_context = bool(state.get("context"))
    has_references = bool(state.get("references"))
    has_final_prompt = bool(state.get("final_prompt"))
    
    # Check if we have a user message
    if not messages:
        # Nothing yet; start coaching
        return "start_coaching"
    if not isinstance(messages[-1], HumanMessage):
        # Last was assistant/system; wait for user input
        return END
    
    # Route based on current step
    if current_step == "awaiting_task_input":
        return "process_task_input"
    elif current_step == "awaiting_context_input":
        return "process_context_input"  
    elif current_step == "awaiting_reference_input":
        return "process_reference_input"
    elif current_step == "awaiting_final_prompt":
        return "process_final_prompt"
    else:
        if has_final_prompt:
            return "agent_node"
        if has_references:
            return "process_final_prompt"
        if has_context:
            return "process_reference_input"
        if has_task:
            return "process_context_input"
        return "process_task_input"

# Router that just routes
def await_user_input_node(state: CoachingState):
    return state

# Graph building
builder = StateGraph(CoachingState)

# Add all nodes
builder.add_node("start_coaching", start_coaching)
builder.add_node("await_user_input", await_user_input_node) 
builder.add_node("process_task_input", process_task_input)
builder.add_node("process_context_input", process_context_input)
builder.add_node("process_reference_input", process_reference_input)
builder.add_node("process_final_prompt", process_final_prompt)
builder.add_node("agent_node", agent_node)
builder.add_node("tool_node", tool_node)
builder.add_node("display_final_result", display_final_result)

# Starting always goes to start_coaching
builder.add_edge(START, "start_coaching")

# From start_coaching, go to router to wait for user input
builder.add_edge("start_coaching", "await_user_input")

# Router conditionally routes to processing nodes based on user input and current step
builder.add_conditional_edges(
    "await_user_input",
    decide_next_step,
    {
        "start_coaching": "start_coaching",
        "process_task_input": "process_task_input",
        "process_context_input": "process_context_input",
        "process_reference_input": "process_reference_input", 
        "process_final_prompt": "process_final_prompt",
        END: END
    }
)

# Each processing node has conditional edges based on success/failure
builder.add_conditional_edges(
    "process_task_input",
    lambda state: "process_context_input" if state.get("current_step") == "awaiting_context_input" else "await_user_input",
    {
        "process_context_input": "process_context_input",
        "await_user_input": "await_user_input"
    }
)

builder.add_conditional_edges(
    "process_context_input", 
    lambda state: "process_reference_input" if state.get("current_step") == "awaiting_reference_input" else "await_user_input",
    {
        "process_reference_input": "process_reference_input",
        "await_user_input": "await_user_input"
    }
)

builder.add_conditional_edges(
    "process_reference_input",
    lambda state: "process_final_prompt" if state.get("current_step") == "awaiting_final_prompt" else "await_user_input",
    {
        "process_final_prompt": "process_final_prompt", 
        "await_user_input": "await_user_input"
    }
)

builder.add_conditional_edges(
    "process_final_prompt",
    lambda state: "agent_node" if state.get("current_step") == "ready_to_refine" else "await_user_input",
    {
        "agent_node": "agent_node",
        "await_user_input": "await_user_input"
    }
)

# Tool execution flow
builder.add_conditional_edges(
    "agent_node",
    should_call_tools,
    {
        "call_tools": "tool_node",
        "display_final_result": "display_final_result"
    }
)

builder.add_edge("tool_node", "display_final_result")
builder.add_edge("display_final_result", END)

# compiling the graph with memory using a persistent connection

# Temporary connection for setup
setup_conn = psycopg.connect(DB_URI)
setup_conn.autocommit = True
PostgresSaver(setup_conn).setup()
setup_conn.close()

# Persistent connection for runtime
conn = psycopg.connect(DB_URI)
memory = PostgresSaver(conn)
coach_graph = builder.compile(checkpointer=memory)

# function for demo Streamlit app
def extract_message_content(message) -> tuple[str, str]:
    if isinstance(message, AIMessage):
        return "assistant", message.content
    elif isinstance(message, HumanMessage):
        return "user", message.content
    elif isinstance(message, SystemMessage):
        return "system", message.content
    elif isinstance(message, tuple):
        return message[0], message[1]
    else:
        return "unknown", str(message)